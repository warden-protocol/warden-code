import "dotenv/config";
import OpenAI from "openai";
import { A2AServer, InMemoryTaskStore } from "@wardenprotocol/agent-kit";
import type { TaskContext, TaskYieldUpdate, MessagePart } from "@wardenprotocol/agent-kit";

const PORT = process.env.PORT || 3000;
const HOST = process.env.HOST || "localhost";
const BASE_URL = `http://${HOST}:${PORT}`;
const STREAMING = {{streaming}};

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const server = new A2AServer({
  agentCard: {
    name: "{{name}}",
    description: "{{description}}",
    url: BASE_URL,
    version: "0.1.0",
    capabilities: {
      streaming: STREAMING,
      multiTurn: {{multiTurn}},
    },
    skills: [{{skills}}],
  },
  taskStore: new InMemoryTaskStore(),
  handler: async function* (context: TaskContext): AsyncGenerator<TaskYieldUpdate> {
    const userMessage = context.message.parts
      ?.filter((p): p is MessagePart & { type: "text" } => p.type === "text")
      .map((p) => p.text)
      .join("\n");

    if (!userMessage) {
      yield {
        state: "completed",
        message: {
          role: "agent",
          parts: [{ type: "text", text: "No message provided." }],
        },
      };
      return;
    }

    try {
      if (STREAMING) {
        yield { state: "working" };

        const stream = await openai.chat.completions.create({
          model: process.env.OPENAI_MODEL || "gpt-4o-mini",
          messages: [
            { role: "system", content: "{{description}}" },
            { role: "user", content: userMessage },
          ],
          stream: true,
        });

        let fullResponse = "";
        for await (const chunk of stream) {
          const content = chunk.choices[0]?.delta?.content || "";
          fullResponse += content;
        }

        yield {
          state: "completed",
          message: {
            role: "agent",
            parts: [{ type: "text", text: fullResponse }],
          },
        };
      } else {
        // Non-streaming: single API call, single response
        yield { state: "working" };

        const response = await openai.chat.completions.create({
          model: process.env.OPENAI_MODEL || "gpt-4o-mini",
          messages: [
            { role: "system", content: "{{description}}" },
            { role: "user", content: userMessage },
          ],
          stream: false,
        });

        const fullResponse = response.choices[0]?.message?.content || "";

        yield {
          state: "completed",
          message: {
            role: "agent",
            parts: [{ type: "text", text: fullResponse }],
          },
        };
      }
    } catch (error) {
      yield {
        state: "failed",
        message: {
          role: "agent",
          parts: [{ type: "text", text: `Error: ${String(error)}` }],
        },
      };
    }
  },
});

server.listen(Number(PORT)).then(() => {
  console.log(`{{name}} running at ${BASE_URL}`);
});
